{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "_wTWzZ-MrJzm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wTWzZ-MrJzm",
    "outputId": "e3ad7780-a1fb-4dae-af87-762d80aa684d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-05 08:08:32--  https://apps.ml.jku.at/challenge/data/datasets/cell_id/images_test.tar\n",
      "Resolving apps.ml.jku.at (apps.ml.jku.at)... 140.78.90.62\n",
      "Connecting to apps.ml.jku.at (apps.ml.jku.at)|140.78.90.62|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 68853760 (66M) [application/x-tar]\n",
      "Saving to: ‘images_test.tar’\n",
      "\n",
      "images_test.tar     100%[===================>]  65.66M  74.2MB/s    in 0.9s    \n",
      "\n",
      "2022-11-05 08:08:33 (74.2 MB/s) - ‘images_test.tar’ saved [68853760/68853760]\n",
      "\n",
      "--2022-11-05 08:08:33--  https://apps.ml.jku.at/challenge/data/datasets/cell_id/images_train.tar\n",
      "Resolving apps.ml.jku.at (apps.ml.jku.at)... 140.78.90.62\n",
      "Connecting to apps.ml.jku.at (apps.ml.jku.at)|140.78.90.62|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 96706560 (92M) [application/x-tar]\n",
      "Saving to: ‘images_train.tar’\n",
      "\n",
      "images_train.tar    100%[===================>]  92.23M  78.6MB/s    in 1.2s    \n",
      "\n",
      "2022-11-05 08:08:34 (78.6 MB/s) - ‘images_train.tar’ saved [96706560/96706560]\n",
      "\n",
      "--2022-11-05 08:08:34--  https://apps.ml.jku.at/challenge/data/datasets/cell_id/y_train.csv\n",
      "Resolving apps.ml.jku.at (apps.ml.jku.at)... 140.78.90.62\n",
      "Connecting to apps.ml.jku.at (apps.ml.jku.at)|140.78.90.62|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 105139 (103K) [text/csv]\n",
      "Saving to: ‘y_train.csv’\n",
      "\n",
      "y_train.csv         100%[===================>] 102.67K  --.-KB/s    in 0.09s   \n",
      "\n",
      "2022-11-05 08:08:35 (1.18 MB/s) - ‘y_train.csv’ saved [105139/105139]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://apps.ml.jku.at/challenge/data/datasets/cell_id/images_test.tar\n",
    "!wget https://apps.ml.jku.at/challenge/data/datasets/cell_id/images_train.tar\n",
    "!wget https://apps.ml.jku.at/challenge/data/datasets/cell_id/y_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "WbggsBgcr9qI",
   "metadata": {
    "id": "WbggsBgcr9qI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "base_dir = './' \n",
    "\n",
    "data_dir = base_dir\n",
    "\n",
    "# path_train = base_dir + 'images_train'\n",
    "# path_test = base_dir + 'images_test'\n",
    "path_train_labels = base_dir + 'y_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BdediXNbrLVt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdediXNbrLVt",
    "outputId": "752b438a-6dd5-40b8-ba15-1a707801ecfb"
   },
   "outputs": [],
   "source": [
    "!mkdir images_test\n",
    "!mkdir images_train\n",
    "!tar -xvf 'images_train.tar' -C images_train\n",
    "!tar -xvf 'images_test.tar' -C images_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "udB-aISdrdhT",
   "metadata": {
    "id": "udB-aISdrdhT"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "classes = {'A549', 'CACO-2', 'HEK 293', 'HeLa', 'MCF7', 'PC-3', 'RT4', 'U-2 OS', 'U-251 MG'}\n",
    "num_classes = len(classes)\n",
    "path_train = \"images_train/images_train\" #\"C:/Users/ASUS/Desktop/7ala - AI/AI in Life Sciences UE/Challenge 3/images_train/\"\n",
    "path_test = \"images_test/images_test\" #\"C:/Users/ASUS/Desktop/7ala - AI/AI in Life Sciences UE/Challenge 3/images_test/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9XWuRc_Kr0N_",
   "metadata": {
    "id": "9XWuRc_Kr0N_"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path, PurePath\n",
    "import pandas as pd\n",
    "from PIL import Image,ImageTk\n",
    "import torchvision.models as models\n",
    "import skimage.io as sk\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dpInhs6hrqTS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpInhs6hrqTS",
    "outputId": "49e1ca41-5cce-41de-afbb-2318ffed667f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9632"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = np.array(pd.read_csv(path_train_labels))\n",
    "train_IDs = train_labels[:,0]\n",
    "train_label = train_labels[:,1]\n",
    "len(train_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "KZMle1F_sD-I",
   "metadata": {
    "id": "KZMle1F_sD-I"
   },
   "outputs": [],
   "source": [
    "test_ids = []\n",
    "for img_filename in os.listdir(path_test):\n",
    "    if img_filename.endswith(\".png\"):\n",
    "        ids = img_filename.split('_')[0]\n",
    "        test_ids.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "JlzQBsflsYUD",
   "metadata": {
    "id": "JlzQBsflsYUD"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "train_imgs = []\n",
    "for ID in train_IDs:\n",
    "        path_red = f'{path_train}/{str(ID).zfill(5)}_red.png'\n",
    "        path_blue = f'{path_train}/{str(ID).zfill(5)}_blue.png'\n",
    "        path_yellow = f'{path_train}/{str(ID).zfill(5)}_yellow.png'\n",
    "        \n",
    "        image_red = cv2.imread(path_red,0)\n",
    "        image_blue = cv2.imread(path_blue,0)\n",
    "        image_yellow = cv2.imread(path_yellow,0)\n",
    "        images = [image_red,image_blue, image_yellow]\n",
    "        input_image = np.stack(images, axis=-1)\n",
    "        train_imgs.append(input_image)\n",
    "test_imgs = []\n",
    "for ID in test_ids:\n",
    "        path_red = f'{path_test}/{str(ID).zfill(5)}_red.png'\n",
    "        path_blue = f'{path_test}/{str(ID).zfill(5)}_blue.png'\n",
    "        path_yellow = f'{path_test}/{str(ID).zfill(5)}_yellow.png'\n",
    "        \n",
    "        image_red = cv2.imread(path_red,0)\n",
    "        image_blue = cv2.imread(path_blue,0)\n",
    "        image_yellow = cv2.imread(path_yellow,0)\n",
    "        images = [image_red,image_blue, image_yellow]\n",
    "        input_image = np.stack(images, axis=-1)\n",
    "        test_imgs.append(input_image)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "SfrHgP5eskhM",
   "metadata": {
    "id": "SfrHgP5eskhM"
   },
   "outputs": [],
   "source": [
    "image_transforms = { \n",
    "    'train': transforms.Compose([\n",
    "        #transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.Resize(size=224),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        \n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        #transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'valid': transforms.Compose([\n",
    "        transforms.Resize(size=224),\n",
    "        #transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "     'test': transforms.Compose([\n",
    "        transforms.Resize(size=224),\n",
    "        #transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tRj2Fx7XsoQY",
   "metadata": {
    "id": "tRj2Fx7XsoQY"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, ids_and_labels, rootDir, sourceTransform):\n",
    "        self.ids = ids_and_labels[:,0]\n",
    "#         print(len(self.ids))\n",
    "        self.data_labels = ids_and_labels[:,1]\n",
    "#         print(self.data_labels)\n",
    "        self.rootDir = rootDir\n",
    "        self.sourceTransform = sourceTransform\n",
    "        train_imgs = []\n",
    "        for ID in self.ids: #train_IDs:\n",
    "            path_red = f'{self.rootDir}/{str(ID).zfill(5)}_red.png'\n",
    "            path_blue = f'{self.rootDir}/{str(ID).zfill(5)}_blue.png'\n",
    "            path_yellow = f'{self.rootDir}/{str(ID).zfill(5)}_yellow.png'\n",
    "\n",
    "            image_red = cv2.imread(path_red,0)\n",
    "            image_blue = cv2.imread(path_blue,0)\n",
    "            image_yellow = cv2.imread(path_yellow,0)\n",
    "            images = [image_red,image_blue, image_yellow]\n",
    "            input_image = np.stack(images, axis=-1)\n",
    "            train_imgs.append(input_image)\n",
    "#         plt.imshow(train_imgs[0])\n",
    "        self.train_imgs = train_imgs\n",
    "#         print(\"hi\",len(self.train_imgs))\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.data_labels))\n",
    "    \n",
    "    def one_hot_encode(self, label):\n",
    "        \n",
    "        self.classes = ['A549', 'CACO-2', 'HEK 293', 'HeLa', 'MCF7', 'PC-3', 'RT4', 'U-2 OS', 'U-251 MG']\n",
    "        mapping = {}\n",
    "        for x in range(len(self.classes)):\n",
    "            mapping[self.classes[x]] = x\n",
    "        '''\n",
    "        onehot_encoded = []\n",
    "        #for value in label:\n",
    "        arr = list(np.zeros(len(self.classes), dtype = int))\n",
    "        arr[mapping[label]] = 1\n",
    "        #onehot_encoded.append(arr)\n",
    "        '''\n",
    "        return torch.tensor(mapping[label])\n",
    "        #return torch.tensor(arr)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()                \n",
    "        label = self.one_hot_encode(self.data_labels[idx])\n",
    "        image = Image.fromarray(self.train_imgs[idx])\n",
    "        \n",
    "        if self.sourceTransform:\n",
    "            image = self.sourceTransform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "XKwN78kLsuLv",
   "metadata": {
    "id": "XKwN78kLsuLv"
   },
   "outputs": [],
   "source": [
    "train_labels1 = train_labels[:7890,:] #[:,0]\n",
    "valid_labels = train_labels[7890:,:] #[:,0]\n",
    "\n",
    "train_trainset = CustomDataset(train_labels1, path_train, image_transforms['train'])\n",
    "valid_trainset = CustomDataset(valid_labels, path_train, image_transforms['valid'])\n",
    "# test_trainset = CustomDataset(valid_labels, path_train, image_transforms['test'])\n",
    "\n",
    "train_data_loader = DataLoader(train_trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "valid_data_loader = DataLoader(valid_trainset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "CDo_yyaP-sv3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CDo_yyaP-sv3",
    "outputId": "37c59fea-bbb7-4300-eb0a-46bc5e337eb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1742"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "JaUGOBY3x-re",
   "metadata": {
    "id": "JaUGOBY3x-re"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ru4oWbf_wm7k",
   "metadata": {
    "id": "ru4oWbf_wm7k"
   },
   "outputs": [],
   "source": [
    "class EfficientNetB0(nn.Module):\n",
    "    def __init__(self, n_units_out):\n",
    "        super(EfficientNetB0, self).__init__()\n",
    "        self.model = torchvision.models.efficientnet_b0(pretrained=True)\n",
    "        \n",
    "        self.model.classifier = nn.Sequential(nn.Dropout(p=0.65, inplace=True),\n",
    "                                              nn.Linear(in_features=1280, out_features=n_units_out, bias=True))\n",
    "    def forward(self, x):\n",
    "        x = self.model.features(x)\n",
    "\n",
    "        x = self.model.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.model.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def get_features(self,x):\n",
    "        x = self.model.features(x)\n",
    "        x = self.model.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bGnoLNIDztRS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bGnoLNIDztRS",
    "outputId": "2401202f-b23b-4454-8e0b-fd2342cb25ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X44-WL9v8_az",
   "metadata": {
    "id": "X44-WL9v8_az"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "_aj0y-MN7tJg",
   "metadata": {
    "id": "_aj0y-MN7tJg"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    classes = torch.argmax(predictions, dim=1)\n",
    "    return torch.mean((classes == labels).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7cBpeQN87Jr6",
   "metadata": {
    "id": "7cBpeQN87Jr6"
   },
   "outputs": [],
   "source": [
    "model = EfficientNetB0(9)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "train_data_size = len(train_data_loader)\n",
    "valid_data_size = len(valid_data_loader)\n",
    "#train_data_size = len(valid_data_loader)\n",
    "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
    "    '''\n",
    "    Function to train and validate\n",
    "    Parameters\n",
    "        :param model: Model to train and validate\n",
    "        :param loss_criterion: Loss Criterion to minimize\n",
    "        :param optimizer: Optimizer for computing gradients\n",
    "        :param epochs: Number of epochs (default=25)\n",
    "  \n",
    "    Returns\n",
    "        model: Trained Model with best validation accuracy\n",
    "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
    "    '''\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    features_ep = []\n",
    "    for epoch in range(epochs):\n",
    "        '''\n",
    "        if epoch >= 5:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "        '''\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))        \n",
    "        model.train()\n",
    "        \n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        \n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_data_loader,0):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)            \n",
    "            loss = loss_criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Compute the accuracy\n",
    "            train_acc += accuracy(outputs, labels)\n",
    "\n",
    "        # Validation - No gradient tracking needed\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            # Validation loop\n",
    "            features = None\n",
    "            for j, (inputs, labels) in enumerate(valid_data_loader,0):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                output_features = model.get_features(inputs)\n",
    "                current_features = output_features.detach().cpu().numpy()\n",
    "                \n",
    "                if features is not None:\n",
    "                    features = np.concatenate((features, current_features))\n",
    "                else:\n",
    "                    features = current_features\n",
    "                \n",
    "                loss = loss_criterion(outputs, labels)\n",
    "\n",
    "                # Compute the total loss for the batch and add it to valid_loss\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                # Calculate validation accuracy\n",
    "                valid_acc += accuracy(outputs, labels)\n",
    "        features_ep.append(features)   \n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss/train_data_size \n",
    "        avg_train_acc = train_acc/train_data_size\n",
    "\n",
    "        # Find average training loss and training accuracy\n",
    "        avg_valid_loss = valid_loss/valid_data_size \n",
    "        avg_valid_acc = valid_acc/valid_data_size\n",
    "\n",
    "        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
    "                \n",
    "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation Loss : {:.4f}, Accuracy: {:.4f}%\".format(epoch+1, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100))\n",
    "\n",
    "        # Save if the model has best accuracy till now\n",
    "        torch.save(model, '_model_'+str(epoch)+'.pt')\n",
    "            \n",
    "    return model, history, np.array(features_ep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "CNjJeD30cfRD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNjJeD30cfRD",
    "outputId": "b1a365eb-4260-4fa4-f0ce-2d80b1551ed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "Epoch : 001, Training: Loss: 66.5256, Accuracy: 21.5081%, \n",
      "\t\tValidation Loss : 53.7138, Accuracy: 47.5974%\n",
      "Epoch: 2/10\n",
      "Epoch : 002, Training: Loss: 46.8865, Accuracy: 52.3097%, \n",
      "\t\tValidation Loss : 33.0588, Accuracy: 69.5049%\n",
      "Epoch: 3/10\n",
      "Epoch : 003, Training: Loss: 29.6757, Accuracy: 70.4875%, \n",
      "\t\tValidation Loss : 19.3330, Accuracy: 81.5828%\n",
      "Epoch: 4/10\n",
      "Epoch : 004, Training: Loss: 19.0975, Accuracy: 80.8761%, \n",
      "\t\tValidation Loss : 12.4791, Accuracy: 87.4919%\n",
      "Epoch: 5/10\n",
      "Epoch : 005, Training: Loss: 13.6420, Accuracy: 86.3290%, \n",
      "\t\tValidation Loss : 8.7447, Accuracy: 91.4854%\n",
      "Epoch: 6/10\n",
      "Epoch : 006, Training: Loss: 10.2610, Accuracy: 89.3050%, \n",
      "\t\tValidation Loss : 7.4910, Accuracy: 92.7841%\n",
      "Epoch: 7/10\n",
      "Epoch : 007, Training: Loss: 7.9408, Accuracy: 91.9914%, \n",
      "\t\tValidation Loss : 6.1020, Accuracy: 93.8636%\n",
      "Epoch: 8/10\n",
      "Epoch : 008, Training: Loss: 6.2454, Accuracy: 93.6994%, \n",
      "\t\tValidation Loss : 5.7447, Accuracy: 94.1883%\n",
      "Epoch: 9/10\n",
      "Epoch : 009, Training: Loss: 5.4271, Accuracy: 94.7776%, \n",
      "\t\tValidation Loss : 5.1172, Accuracy: 94.8295%\n",
      "Epoch: 10/10\n",
      "Epoch : 010, Training: Loss: 4.4604, Accuracy: 95.2584%, \n",
      "\t\tValidation Loss : 4.6426, Accuracy: 95.2110%\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "loss_func = nn.CrossEntropyLoss().to(device) #torchvision.ops.sigmoid_focal_loss().to(device) #\n",
    "num_epochs = 10\n",
    "optimizer = optim.RAdam(model.parameters(), lr=0.0001)\n",
    "trained_model, history, features = train_and_validate(model, loss_func, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7k7ECthvQ9OP",
   "metadata": {
    "id": "7k7ECthvQ9OP"
   },
   "outputs": [],
   "source": [
    "colors_per_class = {\n",
    "    'A549' : [254, 202, 87],\n",
    "    'CACO-2' : [255, 107, 107],\n",
    "    'HEK 293' : [10, 189, 227],\n",
    "    'HeLa' : [255, 159, 243],\n",
    "    'MCF7' : [16, 172, 132],\n",
    "    'PC-3' : [128, 80, 128],\n",
    "    'RT4' : [87, 101, 116],\n",
    "    'U-2 OS' : [52, 31, 151],\n",
    "    'U-251 MG' : [100, 100, 255],\n",
    "    #'9' : [100, 100, 255],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "vXdHeMXSSmTb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vXdHeMXSSmTb",
    "outputId": "2e0db12c-8d94-4054-c530-7efa0937a389"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1742, 1280)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dHvrGFA7EMR0",
   "metadata": {
    "id": "dHvrGFA7EMR0"
   },
   "outputs": [],
   "source": [
    "classes = ['A549', 'CACO-2', 'HEK 293', 'HeLa', 'MCF7', 'PC-3', 'RT4', 'U-2 OS', 'U-251 MG']\n",
    "decode = {}\n",
    "for x in range(len(classes)):\n",
    "    decode[x] = classes[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "g-sFub2qW5KR",
   "metadata": {
    "id": "g-sFub2qW5KR"
   },
   "outputs": [],
   "source": [
    "labels =[]\n",
    "for i in range(len(valid_trainset)):\n",
    "  \n",
    "    label = valid_trainset[i][1].numpy()\n",
    "    images.append(image)\n",
    "    labels.append(decode[int(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c8dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savez_compressed\n",
    "# define data\n",
    "\n",
    "# save to npy file\n",
    "savez_compressed('features.npz', features)\n",
    "np.save('labels.npy',np.array(labels))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "96d504d4fbda4ae5c6efd758aa220ac835734eeef386c18599e3d9cf40372ab4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
